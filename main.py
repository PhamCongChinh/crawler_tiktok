import asyncio
from datetime import datetime, timedelta
import json
import os
from pathlib import Path
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from api import postToESUnclassified
from db.mongo import MongoDB
from post import TikTokPostFlattener
import scraper
import ast
import socket
import aiohttp
from datetime import datetime, timezone

from dotenv import load_dotenv
load_dotenv()
ORG_ID = os.getenv("ORG_ID")
ORGS_ID = ast.literal_eval(ORG_ID)
STATUS = os.getenv("STATUS")
DELAY = int(os.getenv("DELAY"))

output = Path(__file__).parent / "results"
output.mkdir(exist_ok=True)


mongo = MongoDB()
flattener = TikTokPostFlattener()

async def main_job():
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu ch·∫°y job...")
        keywords = mongo.db["facebook_search_keywords"]

        for org in ORGS_ID:
            count = await keywords.count_documents({"org_id": org, "status": STATUS})
            print(f"[{org}]T·ªïng s·ªë keyword t√¨m th·∫•y:", count)

            keywords_list = await keywords.find(
                {"org_id": org, "status": STATUS}
            ).to_list(length=None)
            # Duy·ªát danh s√°ch keyword
            for keyword in keywords_list:
                try:
                    kw = keyword.get("keyword", "")
                    print(f"[{org}]üîç ƒêang x·ª≠ l√Ω keyword: {kw}")

                    # G·ªçi scraper
                    search_data = await scraper.scrape_search(keyword=kw)
                    data = flattener.flatten_batch(search_data)
                    with open(output.joinpath("search.json"), "w", encoding="utf-8") as file:
                        json.dump(data, file, indent=2, ensure_ascii=False)
                    print(f"[{org}]T·ªïng d·ªØ li·ªáu {len(data)}")
                    if (len(data) > 0):
                        # G·ª≠i d·ªØ li·ªáu l√™n Elasticsearch
                        await postToESUnclassified(data)
                        print(f"[{org}]‚úÖ ƒê√£ g·ª≠i th√†nh c√¥ng keyword: {kw}")
                    else:
                        print(f"[{org}]‚úÖ Kh√¥ng c√≥ d·ªØ li·ªáu")

                except Exception as inner_e:
                    print(f"‚ùå L·ªói khi x·ª≠ l√Ω keyword {keyword.get('keyword')}: {inner_e}")

                # Ngh·ªâ gi·ªØa c√°c l·∫ßn x·ª≠ l√Ω ƒë·ªÉ tr√°nh b·ªã rate-limit
                await asyncio.sleep(5)
                break
            print("[{org}]üèÅ Job ho√†n t·∫•t!")
            break
        # ƒê·∫øm s·ªë keyword
        # count = await keywords.count_documents({"org_id": ORG_ID, "status": STATUS})
        # print("T·ªïng s·ªë keyword t√¨m th·∫•y:", count)

        # # L·∫•y to√†n b·ªô d·ªØ li·ªáu tr∆∞·ªõc ƒë·ªÉ tr√°nh l·ªói CursorNotFound
        # keywords_list = await keywords.find(
        #     {"org_id": ORG_ID, "status": STATUS}
        # ).to_list(length=None)

        # # Duy·ªát danh s√°ch keyword
        # for keyword in keywords_list:
        #     try:
        #         kw = keyword.get("keyword", "")
        #         print(f"üîç ƒêang x·ª≠ l√Ω keyword: {kw}")

        #         # G·ªçi scraper
        #         search_data = await scraper.scrape_search(keyword=kw, max_search=48)
        #         data = flattener.flatten_batch(search_data)

        #         # G·ª≠i d·ªØ li·ªáu l√™n Elasticsearch
        #         await postToESUnclassified(data)
        #         print(f"‚úÖ ƒê√£ g·ª≠i th√†nh c√¥ng keyword: {kw}")

        #     except Exception as inner_e:
        #         print(f"‚ùå L·ªói khi x·ª≠ l√Ω keyword {keyword.get('keyword')}: {inner_e}")

        #     # Ngh·ªâ gi·ªØa c√°c l·∫ßn x·ª≠ l√Ω ƒë·ªÉ tr√°nh b·ªã rate-limit
        #     await asyncio.sleep(10)

        # print("üèÅ Job ho√†n t·∫•t!")

    except Exception as e:
        print(f"‚ùå L·ªói trong main_job: {e}")

async def main():
    await mongo.connect()
    print("‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB")

    await main_job()

    scheduler = AsyncIOScheduler()
    scheduler.add_job(
        main_job,
        "interval",
        minutes=DELAY
        # next_run_time=datetime.now()  # ch·∫°y ngay l·∫ßn ƒë·∫ßu
        # next_run_time=datetime.now() + timedelta(seconds=2)
    )
    scheduler.start()
    print("‚úÖ Scheduler started. Waiting for jobs...")

    try:
        await asyncio.Event().wait()  # gi·ªØ ch∆∞∆°ng tr√¨nh ch·∫°y m√£i
    finally:
        print("üßπ ƒêang ƒë√≥ng k·∫øt n·ªëi MongoDB...")
        await mongo.close()

def get_local_ip():
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    s.connect(("8.8.8.8", 80))
    ip = s.getsockname()[0]
    s.close()
    return ip

async def ping_api():
    ts = datetime.now(timezone.utc).strftime("%H:%M:%S %d/%m/%Y")
    async with aiohttp.ClientSession() as session:
        while True:
            try:
                await session.post(
                    "http://222.254.14.6:8100/api/heartbeat/heartbeat",
                    json={
                        "botId": "crawl-node-01",
                        "botType": "Tiktok",
                        "serverIp": get_local_ip(),
                        "lastPingAt": ts,
                        "status": "RUNNING"
                    }
                )
                print("‚ù§Ô∏è Heartbeat sent")
            except Exception as e:
                print("Ping error:", e)

            await asyncio.sleep(10)

async def run_app():
    print("üñ• Local IP:", get_local_ip())

    await asyncio.gather(
        main(),        # bot crawl
        ping_api()     # heartbeat 10s
    )

if __name__ == "__main__":
    try:
        # asyncio.run(main())
        asyncio.run(run_app())
    except KeyboardInterrupt:
        print("\nüõë D·ª´ng ch∆∞∆°ng tr√¨nh theo y√™u c·∫ßu ng∆∞·ªùi d√πng.")